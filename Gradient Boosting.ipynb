{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "332d5c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebook was created with the help of ChatGPT\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e827a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "filtered_df = pd.read_csv('filtered_mental_health_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68a45e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "feature_columns = [col for col in filtered_df.columns if not col.startswith('DSM')]\n",
    "X = filtered_df[feature_columns]\n",
    "y = filtered_df['DSM_MJD'].apply(lambda x: 1 if x == 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f479a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed143f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfb0fea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best CV ROC-AUC Score: 0.7939835385514478\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best CV ROC-AUC Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e18eecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "y_pred_proba = grid_search.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6477ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89       802\n",
      "           1       0.55      0.27      0.36       199\n",
      "\n",
      "    accuracy                           0.81      1001\n",
      "   macro avg       0.70      0.61      0.62      1001\n",
      "weighted avg       0.78      0.81      0.78      1001\n",
      "\n",
      "Test ROC-AUC: 0.7600 ± 0.0345\n",
      "Test Accuracy: 0.7872 ± 0.0335\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "\n",
    "roc_auc_scores = cross_val_score(\n",
    "    grid_search.best_estimator_, X_test, y_test,\n",
    "    cv=5, scoring='roc_auc'\n",
    ")\n",
    "\n",
    "# Calculate cross-validated Accuracy scores\n",
    "accuracy_scores = cross_val_score(\n",
    "    grid_search.best_estimator_, X_test, y_test,\n",
    "    cv=5, scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Compute mean and standard deviation\n",
    "roc_auc_mean = np.mean(roc_auc_scores)\n",
    "roc_auc_std = np.std(roc_auc_scores)\n",
    "\n",
    "accuracy_mean = np.mean(accuracy_scores)\n",
    "accuracy_std = np.std(accuracy_scores)\n",
    "\n",
    "# Print detailed classification report on test set\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Print ROC-AUC and Accuracy with standard deviation\n",
    "print(f\"Test ROC-AUC: {roc_auc_mean:.4f} ± {roc_auc_std:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy_mean:.4f} ± {accuracy_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b240bea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Important Features:\n",
      "       feature  importance\n",
      "119        PH4    0.097139\n",
      "232      PEC52    0.068884\n",
      "333       SA15    0.040146\n",
      "94          M5    0.035889\n",
      "309      NSD1E    0.035313\n",
      "268         M9    0.027730\n",
      "10         PD9    0.026710\n",
      "96   IR1INTRO2    0.022511\n",
      "98        PD1B    0.021600\n",
      "103       PD1G    0.020215\n",
      "10 Least Important Features:\n",
      "    feature  importance\n",
      "554  INC_HI         0.0\n",
      "207    SA1C         0.0\n",
      "206    SA1B         0.0\n",
      "205    SA1A         0.0\n",
      "204    CD18         0.0\n",
      "203   CD16H         0.0\n",
      "202   CD16G         0.0\n",
      "201   CD16F         0.0\n",
      "200   CD16E         0.0\n",
      "199   CD16D         0.0\n"
     ]
    }
   ],
   "source": [
    "#Feature Importance\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "feature_importances_top = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': best_gb_model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Important Features:\")\n",
    "print(feature_importances_top.head(10))\n",
    "\n",
    "feature_importances_bottom = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': best_gb_model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=True)  \n",
    "\n",
    "\n",
    "print(\"10 Least Important Features:\")\n",
    "print(feature_importances_bottom.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2407ed12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features with zero importance: 297\n"
     ]
    }
   ],
   "source": [
    "#Zero Importance Features\n",
    "zero_importance_features = feature_importances[feature_importances['importance'] == 0.0]\n",
    "num_zero_importance = zero_importance_features.shape[0]\n",
    "\n",
    "print(f\"Number of features with zero importance: {num_zero_importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50d53e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "False Negatives (actual=1, predicted=0) top features:\n",
      "       PH4  PEC52  SA15  M5  NSD1E  M9  PD9  IR1INTRO2  PD1B  PD1G\n",
      "1073   -9      5    -7   1      4   3    1         -7     1     1\n",
      "501    -9      5     1  -7      3   1    1         -7     1    -5\n",
      "\n",
      "False Positive (actual=0, predicted=1) top features:\n",
      "       PH4  PEC52  SA15  M5  NSD1E  M9  PD9  IR1INTRO2  PD1B  PD1G\n",
      "1501   -9      5    -7  -7      4   2    5         -7     5     5\n",
      "\n",
      "Mode values of top features for correctly predicted POSITIVE cases:\n",
      " PH4         -9.0\n",
      "PEC52        5.0\n",
      "SA15        -7.0\n",
      "M5          -7.0\n",
      "NSD1E        2.0\n",
      "M9          -7.0\n",
      "PD9         -7.0\n",
      "IR1INTRO2   -7.0\n",
      "PD1B        -7.0\n",
      "PD1G        -7.0\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Mode values of top features for correctly predicted NEGATIVE cases:\n",
      " PH4         -9\n",
      "PEC52        5\n",
      "SA15        -7\n",
      "M5          -7\n",
      "NSD1E        4\n",
      "M9          -7\n",
      "PD9         -7\n",
      "IR1INTRO2   -7\n",
      "PD1B         1\n",
      "PD1G        -5\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import pandas as pd\n",
    "\n",
    "results_df = X_test.copy()\n",
    "results_df['actual'] = y_test.values\n",
    "results_df['predicted'] = y_pred\n",
    "\n",
    "# Top 10 important features\n",
    "top_features = ['PH4', 'PEC52', 'SA15', 'M5', 'NSD1E', \n",
    "                'M9', 'PD9', 'IR1INTRO2', 'PD1B', 'PD1G']\n",
    "\n",
    "# Misclassification examples\n",
    "false_negatives = results_df[(results_df['actual'] == 1) & (results_df['predicted'] == 0)].iloc[[0, 1]]\n",
    "false_positive = results_df[(results_df['actual'] == 0) & (results_df['predicted'] == 1)].iloc[[0]]\n",
    "\n",
    "print(\"\\nFalse Negatives (actual=1, predicted=0) top features:\\n\", false_negatives[top_features])\n",
    "print(\"\\nFalse Positive (actual=0, predicted=1) top features:\\n\", false_positive[top_features])\n",
    "\n",
    "# Compute and print mode separately for correct positives and correct negatives\n",
    "correct_positives = results_df[(results_df['actual'] == 1) & (results_df['predicted'] == 1)]\n",
    "correct_negatives = results_df[(results_df['actual'] == 0) & (results_df['predicted'] == 0)]\n",
    "\n",
    "correct_pos_modes = correct_positives[top_features].mode().iloc[0]\n",
    "correct_neg_modes = correct_negatives[top_features].mode().iloc[0]\n",
    "\n",
    "print(\"\\nMode values of top features for correctly predicted POSITIVE cases:\\n\", correct_pos_modes)\n",
    "print(\"\\nMode values of top features for correctly predicted NEGATIVE cases:\\n\", correct_neg_modes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c3191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
